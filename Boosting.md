Boosting相关概念和算法
=============

1. 概念
* 将弱学习算法提升为强学习算法:
	*  每一轮改变训练数据的权值或概率分布
	*  将弱分类器组合成一个强分类器

--------------
2. 算法
* Adaboost算法
	>特点：加法模型，损失函数是指数函数，学习算法为前向分步算法的二分类学习方法，训练误差以指数速率下降。
	>  属于误分类驱动算法，提高前一轮被错误分类的样本的权值，从而使得后一轮受弱分类器更大关注。
	*  同时加大分类误差小的弱分类器的权值，使它在表决中起较大作用。
	*  步骤： a. 根据前一步得出来的样本权值分布，求当前分类器(加权)的误差。
	*  b.  算出它的系数(分类器权值)，更新新的样本的权值分布。重复迭代
		
-------------------
* 提升树算法 利用残差优化
	>特点：以树为基学习器的前向分步算法
	*  第一次拟合原始数据，之后只需要拟合残差用上次预测结果与训练数据的差作为残差，作为新的训练数据来重新训练。
--------------------------
* GBDT: Gradient Boost Decision Tree  也叫GBRT / Tree Net / MART
	>特点：以CART树为基学习器的前向分步算法
	*  利用损失函数的负梯度在当前模型的值 作为 回归问题提升树中残差的近似值来拟合

